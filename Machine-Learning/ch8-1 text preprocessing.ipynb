{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch.8 텍스트 분선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, \\\n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal. \\\n",
    "it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself \\\n",
    "was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff.\\\n",
    "He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "okt=Okt()  \n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma  \n",
    "kkma=Kkma()  \n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. 정제(Cleaning)와 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수:  179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수: ', len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am actively looking for Ph.D. students. and you are a Ph.D. student.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "actively\n",
      "looking\n",
      "Ph.D.\n",
      "students.\n",
      "Ph.D.\n",
      "student.\n"
     ]
    }
   ],
   "source": [
    "for word in text.split() :\n",
    "    if word not in stopwords :\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. 어간 추출(Stemming) 및 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', \n",
    "       'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])    # dies를 dy로 watched를 watched로 하는 실수를 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')    # 품사를 지정해주면 다시 잘 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer    # 어간을 추출하는 함수\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming                     으로 추출하면 어간을 추출\n",
    "#am → am\n",
    "#the going → the go\n",
    "#having → hav\n",
    "\n",
    "#Lemmatization               으로 추출하면 어간을 표제어로 추출\n",
    "#am → be\n",
    "#the going → the going\n",
    "#having → have\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens :\n",
    "    if w not in stop_words :\n",
    "        result.append(w)\n",
    "        \n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')  # 정규 표현식\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=re.compile(\"a.c\")       # a와 c 사이에 임의의 문자가 있다 하는 것\n",
    "r.search(\"kkk\")           # a와 c가 없기에 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")           # 매치가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"ab?c\")      # b?는 b가 있을 수도 있고 없을 수 있다 판단\n",
    "r.search(\"abbc\")          # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")           # b가 하나 있다고 하니 매치가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"ab*c\")\n",
    "r.search(\"a\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=re.compile(\"ab.\")\n",
    "\n",
    "r.search(\"kkkabc\")  # 매치되는 위치를 서치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.match(\"kkkabc\")  # 사이에 있으면 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.match(\"abckkk\")  # 앞자리에 있으면 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "\n",
    "re.split(\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과+딸기+수박+메론+바나나\"\n",
    "\n",
    "re.split(\"\\+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과1딸기2수박5메론9바나나\"\n",
    "\n",
    "re.split(\"[0-9]\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과1딸기2수박-메론=바나나\"\n",
    "\n",
    "re.split(\"[0-9-=+]\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과 딸기 수박 메론 바나나']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "\n",
    "re.split(\"\\+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"이름 : 김철수 \\\n",
    "전화번호 : 010 - 1234 - 1234 \\\n",
    "나이 : 30 \\\n",
    "성별 : 남\"\n",
    "    \n",
    "re.findall(\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp    sometimes called a rational expression        is  in theoretical computer scienceand formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Regular expression : A regular expression, regex or regexp[1]\\\n",
    "(sometimes called a rational expression)[2][3] is, in theoretical computer science\\\n",
    "and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "re.sub('[^a-zA-Z]',' ',text)\n",
    "# sub() 함수는 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression : A regular expression regex or regexpsometimes called a rational expression is in theoretical computer scienceand formal language theory a sequence of characters that define a search pattern'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Regular expression : A regular expression, regex or regexp[1]\\\n",
    "(sometimes called a rational expression)[2][3] is, in theoretical computer science\\\n",
    "and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "re.sub('[^a-zA-Z :]','',text) # 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"100 John    PROF \\\n",
    "101 James   STUD \\\n",
    "102 Mac   STUD\"\n",
    "\n",
    "re.split('\\s+', text)     # '\\s+'는 공백을 찾아내는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+', text)   # 숫자만 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]',text)  # 대문자만 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2vec 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04,  1.5335083e-03,\n",
       "        1.1062622e-03, -1.4038086e-03, -3.0517578e-05, -4.1961670e-04,\n",
       "       -5.7601929e-04,  1.0757446e-03, -1.0223389e-03, -6.1798096e-04,\n",
       "       -7.5531006e-04,  1.4038086e-03, -1.6403198e-03, -6.3323975e-04,\n",
       "        1.6326904e-03, -1.0070801e-03, -1.2664795e-03,  6.5231323e-04,\n",
       "       -4.1580200e-04, -1.0757446e-03,  1.5258789e-03, -2.7465820e-04,\n",
       "        1.4019012e-04,  1.5716553e-03,  1.3580322e-03, -8.3160400e-04,\n",
       "       -1.4038086e-03,  1.5792847e-03,  2.5367737e-04, -7.3242188e-04,\n",
       "       -1.0538101e-04, -1.1672974e-03,  1.5792847e-03,  6.5612793e-04,\n",
       "       -6.5994263e-04,  2.9206276e-06,  1.1291504e-03,  4.2724609e-04,\n",
       "       -3.7002563e-04, -1.1520386e-03,  1.2664795e-03, -3.5166740e-06,\n",
       "        2.6512146e-04, -4.0245056e-04,  1.4114380e-04, -3.3617020e-05,\n",
       "        7.5912476e-04, -5.1879883e-04, -7.1048737e-05,  6.0272217e-04,\n",
       "       -5.0735474e-04, -1.6250610e-03, -4.3678284e-04, -9.9182129e-04,\n",
       "       -1.2207031e-03, -3.2234192e-04,  6.8664551e-05, -1.1672974e-03,\n",
       "       -5.1116943e-04,  1.4114380e-03,  3.3569336e-04, -4.7492981e-04,\n",
       "       -1.3732910e-03,  3.6621094e-04, -1.4419556e-03, -6.0653687e-04,\n",
       "        8.0108643e-04,  1.1291504e-03, -8.3541870e-04, -1.1596680e-03,\n",
       "        9.1552734e-04,  5.2261353e-04, -3.2806396e-04,  1.5945435e-03,\n",
       "       -1.5792847e-03, -3.5667419e-04,  4.9591064e-04,  1.0147095e-03,\n",
       "       -1.0986328e-03, -1.6593933e-04, -1.4209747e-04, -2.6130676e-04,\n",
       "        1.2588501e-03,  3.8623810e-05,  1.6880035e-04, -1.0299683e-03,\n",
       "        1.6098022e-03,  6.2942505e-04,  4.1770935e-04, -1.3504028e-03,\n",
       "        3.4904480e-04,  1.1444092e-03, -1.2054443e-03, -1.1825562e-03,\n",
       "        9.4985962e-04,  6.0558319e-05,  1.0728836e-05, -6.6757202e-04,\n",
       "        1.2435913e-03,  6.9046021e-04,  5.5551529e-05, -8.6212158e-04,\n",
       "       -1.1672974e-03,  1.2130737e-03, -8.0490112e-04, -8.7738037e-04,\n",
       "        2.2792816e-04, -3.9672852e-04, -8.5830688e-04,  2.8800964e-04,\n",
       "       -1.5869141e-03,  4.8446655e-04, -1.1215210e-03,  1.9669533e-06,\n",
       "       -3.7956238e-04,  7.0571899e-04, -1.5869141e-03,  1.6250610e-03,\n",
       "        1.5563965e-03, -4.3106079e-04,  9.8419189e-04,  9.0408325e-04,\n",
       "       -1.3961792e-03,  1.2054443e-03, -7.0190430e-04,  2.7084351e-04,\n",
       "       -1.2359619e-03,  6.9046021e-04, -8.4304810e-04,  1.3427734e-03,\n",
       "       -1.4343262e-03, -6.7138672e-04,  1.5487671e-03, -1.0986328e-03,\n",
       "        1.1901855e-03, -1.4266968e-03, -6.8283081e-04, -7.8582764e-04,\n",
       "        4.8065186e-04,  4.0817261e-04, -6.3705444e-04,  1.4495850e-04,\n",
       "       -9.7656250e-04,  1.4495850e-03,  8.4564090e-07, -1.6632080e-03,\n",
       "       -3.2806396e-04,  6.2942505e-04, -1.4343262e-03, -3.4141541e-04,\n",
       "        1.1520386e-03, -5.3024292e-04, -4.7111511e-04, -8.5067749e-04,\n",
       "       -1.3809204e-03, -1.2435913e-03, -1.3275146e-03,  1.0757446e-03,\n",
       "        1.3198853e-03, -3.0326843e-04, -3.7431717e-05,  1.1825562e-03,\n",
       "       -1.3580322e-03, -1.0452271e-03,  5.6743622e-05, -1.0147095e-03,\n",
       "        4.3296814e-04, -1.5716553e-03, -9.1075897e-05,  1.0604858e-03,\n",
       "       -6.0272217e-04, -1.5335083e-03, -1.5335083e-03,  5.4168701e-04,\n",
       "        1.3351440e-03,  4.1198730e-04, -3.1089783e-04,  1.7642975e-04,\n",
       "       -1.3732910e-04, -6.9808960e-04, -8.6212158e-04, -1.0833740e-03,\n",
       "       -2.9802322e-05,  8.0108643e-04,  6.7901611e-04,  3.3569336e-04,\n",
       "       -1.3885498e-03,  1.3504028e-03,  2.3460388e-04, -1.3351440e-03,\n",
       "       -8.7356567e-04, -7.4386597e-04,  1.0833740e-03,  6.0558319e-05,\n",
       "       -1.2664795e-03,  1.1901855e-03, -6.2179565e-04,  1.3597310e-07,\n",
       "        1.2741089e-03, -9.8419189e-04, -1.5487671e-03,  1.5563965e-03,\n",
       "       -1.3122559e-03, -7.9345703e-04,  1.5335083e-03,  1.2969971e-03,\n",
       "       -1.8024445e-04,  9.1934204e-04,  1.2054443e-03,  7.7056885e-04,\n",
       "       -1.6555786e-03,  7.7056885e-04,  1.4495850e-03, -1.3046265e-03,\n",
       "        6.1035156e-04,  6.5994263e-04,  1.2588501e-03,  1.4190674e-03,\n",
       "       -1.2207031e-03, -1.5106201e-03,  1.1291504e-03,  1.3427734e-03,\n",
       "        1.6632080e-03, -5.7220459e-04, -5.5694580e-04,  3.9863586e-04,\n",
       "       -2.7084351e-04,  4.9591064e-04,  1.6098022e-03, -7.0571899e-04,\n",
       "        6.2561035e-04, -9.7656250e-04, -1.8978119e-04,  9.5367432e-05,\n",
       "       -5.1879883e-04, -2.0408630e-04, -8.2778931e-04, -1.2302399e-04,\n",
       "        7.6293945e-04,  3.2234192e-04, -1.2435913e-03,  9.9182129e-04,\n",
       "        1.0604858e-03, -1.4114380e-03,  9.6797943e-05, -1.5563965e-03,\n",
       "        2.1934509e-04, -5.5313110e-05, -9.1171265e-04, -1.4877319e-03,\n",
       "        1.3656616e-03, -8.4304810e-04, -4.1961670e-04,  3.2424927e-04,\n",
       "       -1.0070801e-03,  1.2588501e-04, -4.5585632e-04,  1.9264221e-04,\n",
       "       -2.6893616e-04,  1.4953613e-03, -1.5869141e-03,  5.9127808e-04,\n",
       "       -1.4648438e-03,  9.6511841e-04, -1.2817383e-03,  1.6021729e-03,\n",
       "        1.0910034e-03, -1.3122559e-03,  1.0910034e-03, -5.1116943e-04,\n",
       "        3.4523010e-04,  1.0452271e-03, -2.0694733e-04,  9.0408325e-04,\n",
       "        6.6757202e-04,  1.1062622e-03, -8.7356567e-04, -3.7574768e-04,\n",
       "       -2.5749207e-04, -9.1552734e-05,  1.4343262e-03, -1.1825562e-03,\n",
       "       -8.7261200e-05,  1.3275146e-03, -1.5830994e-04,  1.2893677e-03,\n",
       "       -9.8419189e-04, -5.4931641e-04, -1.5487671e-03,  1.3732910e-03,\n",
       "       -6.0796738e-05, -8.2397461e-04,  1.3275146e-03,  1.1596680e-03,\n",
       "        5.6838989e-04, -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('king', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31618136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('queen', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510957"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_model = gensim.models.Word2Vec.load('./model/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ko_model.wv.most_similar(\"강아지\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('고양이', 0.7290453314781189),\n",
       " ('거위', 0.7185634970664978),\n",
       " ('토끼', 0.7056223750114441),\n",
       " ('멧돼지', 0.6950401067733765),\n",
       " ('엄마', 0.693433403968811),\n",
       " ('난쟁이', 0.6806551218032837),\n",
       " ('한마리', 0.6770296096801758),\n",
       " ('아가씨', 0.675035297870636),\n",
       " ('아빠', 0.6729634404182434),\n",
       " ('목걸이', 0.6512461304664612)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('고양이', 0.7290453314781189), ('거위', 0.7185634970664978), ('토끼', 0.7056223750114441), ('멧돼지', 0.6950401067733765), ('엄마', 0.693433403968811), ('난쟁이', 0.6806551218032837), ('한마리', 0.6770296096801758), ('아가씨', 0.675035297870636), ('아빠', 0.6729634404182434), ('목걸이', 0.6512461304664612)]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
